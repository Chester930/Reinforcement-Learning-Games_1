# 🎮 強化學習遊戲平台 - 完整使用說明書

> 一個專為學習強化學習而設計的互動式教學平台，讓你從零開始掌握 Q-Learning 和 SARSA 演算法

## 📋 目錄
- [📖 專案介紹](#專案介紹)
- [🎯 學習目標](#學習目標)
- [🧠 強化學習基礎知識](#強化學習基礎知識)
- [⚡ 快速開始](#快速開始)
- [🎲 如何使用平台](#如何使用平台)
- [📊 演算法教學](#演算法教學)
- [🛠️ 進階功能](#進階功能)
- [💡 常見問題與解答](#常見問題與解答)
- [📚 延伸學習資源](#延伸學習資源)

---

## 📖 專案介紹

### 🎯 專案目的

本平台專為以下目標而設計：

1. **📚 教育導向**：提供視覺化的強化學習環境，讓複雜的演算法變得易懂
2. **🎮 互動學習**：透過 Grid World 遊戲，讓你親手體驗 AI 如何學習
3. **🔬 實驗平台**：支援參數調整與比較，幫助理解不同設定的影響
4. **📈 成果視覺化**：自動生成學習曲線和 AI 分析報告
5. **🌟 無門檻入門**：提供完整的 Web 介面，不需要程式基礎也能使用

### 👥 適用對象

- **🎓 學生**：想要學習強化學習的初學者
- **👨‍🏫 教師**：需要教學工具的教育工作者
- **🔬 研究者**：想要快速實驗演算法的研究人員
- **💻 開發者**：對 AI 有興趣的程式設計師
- **🤖 AI 愛好者**：想要了解機器學習的任何人

---

## 🎯 學習目標

使用本平台後，你將能夠：

### 🔰 基礎理解
- ✅ 理解強化學習的基本概念（狀態、動作、獎勵）
- ✅ 區分 Q-Learning 和 SARSA 的差異
- ✅ 掌握學習率、探索率等超參數的意義

### 🔄 實作技能
- ✅ 設計並測試自己的地圖環境
- ✅ 調整參數來觀察演算法行為變化
- ✅ 分析訓練結果和學習曲線

### 📊 分析能力
- ✅ 解讀 Q-Table 數值的含義
- ✅ 評估演算法的收斂性和穩定性
- ✅ 比較不同演算法的表現

---

## 🧠 強化學習基礎知識

### 🤔 什麼是強化學習？

強化學習是機器學習的一個分支，其目標是讓 **智能體（Agent）** 在 **環境（Environment）** 中學習如何採取最佳行動。

```
🤖 智能體 ↔️ 🌍 環境

智能體觀察環境狀態 → 選擇行動 → 獲得獎勵 → 更新策略
```

### 🏗️ 核心概念

| 概念 | 說明 | 在我們平台中的例子 |
|------|------|-------------------|
| **狀態 (State)** | 環境的當前情況 | 智能體在格子世界中的位置 |
| **動作 (Action)** | 智能體可以執行的行為 | 上、下、左、右移動 |
| **獎勵 (Reward)** | 執行動作後獲得的回饋 | 到達目標 +10，撞牆 -1 |
| **策略 (Policy)** | 決定在每個狀態該做什麼 | Q-Table 中的最佳動作選擇 |

### 🎮 Grid World 環境說明

我們的平台使用 **Grid World**（格子世界）作為學習環境：

```
🏃 S = 起點 (Start)     🏆 G = 終點 (Goal)
🧱 1 = 障礙物 (Wall)    ⬜ 0 = 可通行 (Free)
```

**範例地圖：**
```
S  0  G
0  1  0  
0  0  0
```

**目標：** 智能體從起點 S 找到最短路徑到達終點 G，同時避開障礙物。

---

## ⚡ 快速開始

### 🐳 Docker 一鍵啟動（推薦）

#### 🚀 最簡單的啟動方式
```bash
# Windows 用戶
start.bat

# Linux/macOS 用戶
./start.sh
```

**只需要安裝 Docker Desktop，然後雙擊執行腳本即可！**

#### 📋 前置需求
1. **安裝 Docker Desktop**
   - Windows/macOS: https://www.docker.com/products/docker-desktop/
   - Linux: `sudo apt-get install docker.io docker-compose`

2. **啟動 Docker Desktop**
   - Windows/macOS: 雙擊 Docker Desktop 圖標
   - Linux: `sudo systemctl start docker`

#### 🎯 啟動後訪問
- **主要介面**: http://localhost:8080
- **API 端點**: http://localhost:8000

#### 🛑 停止服務
```bash
# Windows 用戶
stop.bat

# Linux/macOS 用戶
./stop.sh
```

### 🛠️ 傳統環境設定（5 分鐘）

#### 0️⃣ 環境檢查（可選）
在開始之前，你可以運行環境檢查腳本來確認所有設定都正確：

```bash
python check_environment.py
```

#### 1️⃣ 安裝 Python 環境
```bash
# 建立虛擬環境
python -m venv venv

# 啟動虛擬環境
# Windows 用戶：
.\venv\Scripts\Activate
# macOS/Linux 用戶：
source venv/bin/activate

# 安裝必要套件
pip install -r requirements.txt
```

#### 2️⃣ 啟動後端服務
```bash
# 啟動 API 服務（保持此終端機開啟）
python -m uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

#### 3️⃣ 啟動前端介面
```bash
# 開啟新的終端機，進入前端目錄
cd frontend

# 安裝前端套件
npm install

# 啟動前端服務
npm start
```

#### 4️⃣ 開始使用
打開瀏覽器，前往 `http://localhost:3000`

🎉 **恭喜！你已經成功啟動平台了！**

### 🚀 快速啟動（Windows 用戶）
如果你使用 Windows，可以使用提供的批處理檔案：

```bash
# 啟動後端服務
start_backend.bat

# 啟動前端服務（在新的終端機視窗中）
start_frontend.bat
```

### ⚠️ 重要提醒
- **後端啟動**：使用 `python -m uvicorn` 而不是直接使用 `uvicorn` 命令
- **套件安裝**：確保所有套件都已正確安裝，特別是 `python-multipart`
- **訓練任務**：首次使用時，建議先進行一次 AI 訓練，這樣分析功能才能正常運作
- **分析功能**：只有完成訓練後，學習曲線、熱力圖和最優路徑功能才能正常顯示

---

## 🎲 如何使用平台

### 🏠 首頁導覽

平台提供六大功能模組：

| 模組 | 功能 | 適合對象 |
|------|------|----------|
| 🎮 **手動遊玩** | 親自體驗遊戲，理解環境 | 所有使用者 |
| 🗺️ **地圖管理** | 建立和編輯訓練地圖 | 進階使用者 |
| 🤖 **AI 訓練** | 設定參數並訓練 AI | 學習者 |
| 📊 **AI 分析** | 檢視訓練結果和分析 | 研究者 |
| 🎯 **路徑模擬** | 測試訓練好的 AI 表現 | 教師、學生 |
| ⚙️ **系統設定** | 調整平台參數 | 管理員 |

### 🎓 建議學習流程

#### 🔰 新手入門（第 1-2 天）
1. **🎮 手動遊玩** → 熟悉遊戲規則
2. **🗺️ 建立簡單地圖** → 設計 3x3 的基礎地圖
3. **🤖 第一次 AI 訓練** → 使用預設參數訓練（**重要：確保訓練完成**）
4. **📊 查看結果** → 觀察學習曲線、熱力圖和最優路徑
5. **🔍 分析報告** → 查看 AI 生成的詳細分析報告

#### 🔄 進階實驗（第 3-5 天）
1. **🧪 參數實驗** → 調整學習率、探索率
2. **🆚 演算法比較** → 同時測試 Q-Learning 和 SARSA
3. **🗺️ 複雜地圖** → 設計有多個障礙物的地圖
4. **📈 深度分析** → 解讀 AI 分析報告

#### 🚀 專家應用（第 6+ 天）
1. **🔬 自定義環境** → 設計特殊的挑戰地圖
2. **📊 性能優化** → 找出最佳參數組合
3. **🎯 實際應用** → 將學到的概念應用到其他問題

---

## 📊 演算法教學

### 🧮 Q-Learning 演算法

#### 💡 核心思想
Q-Learning 是一種 **離策略（Off-Policy）** 方法，它學習的是最優策略，而不必遵循當前策略。

#### 🔢 Q-Learning 公式
```
Q(s,a) ← Q(s,a) + α[r + γ·max(Q(s',a')) - Q(s,a)]
```

**參數說明：**
- `Q(s,a)` = 在狀態 s 執行動作 a 的價值
- `α` = 學習率（0-1）
- `r` = 即時獎勵
- `γ` = 折扣因子（0-1）
- `max(Q(s',a'))` = 下一狀態的最大 Q 值

#### ⚡ 特點
- ✅ **快速收斂**：通常學習速度較快
- ✅ **最優解**：理論上會收斂到最優策略
- ⚠️ **過度樂觀**：可能在探索不足時過度自信

### 🔄 SARSA 演算法

#### 💡 核心思想
SARSA 是一種 **同策略（On-Policy）** 方法，它學習的是當前正在執行的策略。

#### 🔢 SARSA 公式
```
Q(s,a) ← Q(s,a) + α[r + γ·Q(s',a') - Q(s,a)]
```

**與 Q-Learning 的差異：**
- Q-Learning 使用 `max(Q(s',a'))` （最佳可能動作）
- SARSA 使用 `Q(s',a')` （實際執行的動作）

#### ⚡ 特點
- ✅ **保守穩定**：學習過程更加穩定
- ✅ **安全性**：在危險環境中表現更好
- ⚠️ **收斂較慢**：需要更多訓練回合

### 🆚 演算法比較

| 特性 | Q-Learning | SARSA |
|------|------------|-------|
| 學習方式 | 離策略 | 同策略 |
| 收斂速度 | 較快 | 較慢 |
| 最終表現 | 可能更優 | 較保守 |
| 穩定性 | 可能振盪 | 較穩定 |
| 適用場景 | 簡單環境 | 複雜/危險環境 |

### 🎛️ 重要參數解析

#### 📈 學習率 (Learning Rate, α)
- **作用**：控制新資訊的接受程度
- **範圍**：0.01 - 0.9
- **建議**：
  - 🐌 `0.01-0.1`：穩定但緩慢
  - ⚡ `0.3-0.5`：平衡選擇
  - 🚀 `0.7-0.9`：快速但可能不穩定

#### 🔍 探索率 (Epsilon, ε)
- **作用**：平衡探索新路徑與利用已知最佳路徑
- **策略**：通常採用衰減機制
- **建議**：
  - 🌟 初始值：`1.0`（100% 探索）
  - 📉 衰減率：`0.995`
  - 🎯 最小值：`0.01`（1% 探索）

#### ⏰ 折扣因子 (Discount Factor, γ)
- **作用**：決定未來獎勵的重要性
- **範圍**：0-1
- **建議**：
  - 🎯 `0.9-0.99`：重視長期目標
  - ⚡ `0.5-0.8`：平衡短期和長期
  - 🏃 `0.1-0.4`：重視即時獎勵

---

## 🛠️ 進階功能

### 🗺️ 自定義地圖設計

#### 🎨 地圖設計原則
1. **🏃 起點設置**：每個地圖必須有一個起點 'S'
2. **🏆 目標設置**：每個地圖必須有一個終點 'G'
3. **🧱 障礙物配置**：適量的障礙物增加挑戰性
4. **📏 大小考量**：建議 3x3 到 10x10 之間

#### 🎯 設計技巧
```
💡 簡單地圖（新手適用）：
S 0 G
0 1 0
0 0 0

🔥 中等難度：
S 0 1 0 G
0 1 1 0 0
0 0 1 0 0
1 0 0 0 0
0 0 0 1 0

🚀 困難地圖：
S 1 0 1 0
0 1 0 1 0
0 0 0 1 0
1 0 1 1 0
0 0 0 0 G
```

### 📈 訓練結果分析

#### 📊 學習曲線解讀
- **📈 上升趨勢**：AI 正在學習並改善
- **📉 下降段**：可能是探索新路徑導致的暫時性能下降
- **📏 平穩段**：演算法已經收斂

#### 🎯 Q-Table 數值分析
- **正值**：該動作被認為是有益的
- **負值**：該動作可能導致懲罰
- **數值大小**：反映動作的相對價值

### 🔧 系統設定優化

#### ⚙️ API 設定
```json
{
  "openai_api_key": "your-api-key",
  "max_analysis_length": 5000,
  "default_episodes": 500,
  "default_learning_rate": 0.1
}
```

#### 🎛️ 訓練參數模板
```json
{
  "快速測試": {
    "episodes": 100,
    "learning_rate": 0.3,
    "epsilon": 0.5
  },
  "標準訓練": {
    "episodes": 500,
    "learning_rate": 0.1,
    "epsilon": 1.0
  },
  "深度訓練": {
    "episodes": 2000,
    "learning_rate": 0.05,
    "epsilon": 1.0
  }
}
```

---

## 💡 常見問題與解答

### 🔧 技術問題

#### ❓ **Q1: 無法啟動服務，出現端口被佔用錯誤**
```bash
# 檢查端口使用情況
netstat -ano | findstr :8000  # Windows
lsof -i :8000  # macOS/Linux

# 使用其他端口
python -m uvicorn main:app --reload --port 8001
```

#### ❓ **Q1.1: uvicorn 命令無法識別**
```bash
# 解決方案：使用 python -m uvicorn
python -m uvicorn main:app --reload --host 0.0.0.0 --port 8000

# 或者安裝 python-multipart
pip install python-multipart
```

#### ❓ **Q2: 前端無法連接到後端 API**
**解決方案：**
1. 確認後端服務運行在 `http://localhost:8000`
2. 檢查防火牆設定
3. 確認瀏覽器沒有阻擋本地連接

#### ❓ **Q3: 訓練結果不如預期**
**可能原因：**
- 🎛️ 學習率設置不當
- 🔍 探索率衰減太快
- 📊 訓練回合數不足
- 🗺️ 地圖設計過於複雜

#### ❓ **Q3.1: 分析功能顯示 404 錯誤**
**解決方案：**
1. 確保已經完成至少一次 AI 訓練
2. 檢查 job 目錄中是否有 `log.csv` 和 `q_table.csv` 檔案
3. 重新進行訓練，確保訓練過程完整完成
4. 如果問題持續，嘗試清除舊的 job 目錄並重新訓練

### 🎓 學習問題

#### ❓ **Q4: Q-Learning 和 SARSA 結果差異很大**
**這是正常現象！** 原因包括：
- 演算法本質不同（離策略 vs 同策略）
- 探索策略的影響
- 隨機性因素

#### ❓ **Q5: AI 找不到最短路徑**
**建議檢查：**
1. 🎯 目標是否可達
2. 🏃 起點位置是否合理
3. 📈 訓練回合是否足夠
4. 🎛️ 參數設置是否適當

#### ❓ **Q6: 學習曲線不收斂**
**可能的解決方案：**
- 📉 降低學習率（如從 0.3 改為 0.1）
- 🔍 調整探索率衰減
- 📊 增加訓練回合數
- 🗺️ 簡化地圖複雜度

### 📚 使用指導

#### ❓ **Q7: 新手應該從哪裡開始？**
**建議流程：**
1. 🎮 手動遊玩幾次，熟悉遊戲
2. 🗺️ 使用預設地圖進行第一次訓練
3. 📊 觀察學習曲線和結果
4. 🔧 嘗試調整單一參數
5. 🆚 比較不同演算法

#### ❓ **Q8: 如何設計有效的教學示範？**
**教學技巧：**
- 📝 從概念解釋開始
- 🎮 讓學生先手動遊玩
- 📊 展示簡單地圖的訓練過程
- 🔍 逐步增加複雜度
- 🎯 強調參數調整的影響

---

## 📚 延伸學習資源

### 📖 推薦書籍
1. **《強化學習導論》** by Sutton & Barto
   - 📘 經典教科書，理論基礎深厚
2. **《動手學強化學習》** by 張偉楠等
   - 🛠️ 實作導向，適合程式設計師

### 🌐 線上資源
1. **OpenAI Gym** - https://gym.openai.com/
   - 🎮 更多強化學習環境
2. **Stable Baselines3** - https://stable-baselines3.readthedocs.io/
   - 🚀 先進的強化學習演算法庫
3. **CS294 Deep RL** - UC Berkeley
   - 🎓 知名大學課程

### 🎥 影片教程
1. **David Silver 的強化學習課程**
   - 🎯 DeepMind 研究員的經典課程
2. **3Blue1Brown 神經網路系列**
   - 🧠 視覺化解釋機器學習概念

### 🔬 進階主題
- **深度 Q 網路 (DQN)**
- **政策梯度方法**
- **演員-評論家方法**
- **多智能體強化學習**

---

## 🏆 專案成果

使用本平台，你將能夠：

### 📊 學習成果
- ✅ 深入理解強化學習核心概念
- ✅ 掌握 Q-Learning 和 SARSA 演算法
- ✅ 具備調試和優化 AI 的能力
- ✅ 培養分析和解決問題的思維

### 🎓 實際技能
- ✅ 設計和測試學習環境
- ✅ 參數調整和性能優化
- ✅ 結果分析和報告撰寫
- ✅ 教學和知識分享

### 🚀 未來發展
- 🔬 進階強化學習研究
- 🎮 遊戲 AI 開發
- 🤖 自動化系統設計
- 📈 數據科學應用

---

## 📞 技術支援

### 🐛 回報問題
如果遇到技術問題，請提供：
1. 錯誤訊息截圖
2. 操作步驟描述
3. 系統環境資訊
4. 使用的參數設定

### 💡 功能建議
歡迎提出改進建議：
- 新的演算法支援
- 介面優化建議
- 教學內容改進
- 新功能需求

### 🤝 社群支持
- 📧 Email: support@rl-platform.com
- 💬 Discord: RLPlatform Community
- 📱 GitHub: https://github.com/your-repo/rl-platform

---

## 🙏 致謝

感謝所有為強化學習教育付出努力的研究者和開發者，特別是：
- Richard S. Sutton 和 Andrew G. Barto（強化學習理論奠基者）
- OpenAI 團隊（開源工具和環境）
- 所有測試和回饋的使用者

---

**🎉 恭喜你完成了強化學習平台的完整學習之旅！現在就開始你的 AI 探索吧！**

> 💡 **記住**：學習 AI 最好的方法就是動手實作。不要害怕實驗，每次失敗都是邁向成功的一步！ 