import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼å¾Œç«¯ï¼Œé¿å…GUIå•é¡Œ
from fastapi import FastAPI, HTTPException, Body
from fastapi.responses import JSONResponse
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import base64
from io import BytesIO
import json
import re
import requests
import markdown2
from datetime import datetime
import subprocess

app = FastAPI()
JOBS_DIR = 'jobs'

@app.get('/{job_id}/curve')
def get_learning_curve(job_id: str):
    log_path = os.path.join(JOBS_DIR, job_id, 'log.csv')
    if not os.path.exists(log_path):
        raise HTTPException(status_code=404, detail='Log not found')
    df = pd.read_csv(log_path)
    rewards = df.groupby('episode')['reward'].sum().tolist()
    steps = df.groupby('episode')['step'].max().tolist()
    return {"rewards": rewards, "steps": steps}

@app.get('/{job_id}/heatmap')
def get_qtable_heatmap(job_id: str):
    qtable_path = os.path.join(JOBS_DIR, job_id, 'q_table.csv')
    if not os.path.exists(qtable_path):
        raise HTTPException(status_code=404, detail='Q-Table not found')
    
    df = pd.read_csv(qtable_path)
    
    # æª¢æŸ¥ Q-Table æ˜¯å¦ç‚ºç©º
    if df.empty:
        raise HTTPException(status_code=400, detail='Q-Table is empty')
    
    # æª¢æŸ¥å¿…è¦çš„æ¬„ä½
    required_columns = ['state', 'action', 'value']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        raise HTTPException(status_code=400, detail=f'Q-Table missing columns: {missing_columns}')
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ç©ºå€¼
    if df[required_columns].isnull().any().any():
        raise HTTPException(status_code=400, detail='Q-Table contains null values')
    
    # ä»¥ state ç‚º row, action ç‚º column, value ç‚º cell
    try:
        pivot = df.pivot(index='state', columns='action', values='value').fillna(0)
        
        # æª¢æŸ¥ pivot çµæœæ˜¯å¦ç‚ºç©º
        if pivot.empty:
            raise HTTPException(status_code=400, detail='Q-Table pivot result is empty')
            
    except ValueError as e:
        raise HTTPException(status_code=400, detail=f'Q-Table pivot failed: {str(e)}')
    
    plt.figure(figsize=(8, 6))
    plt.title('Q-Table Heatmap')
    plt.imshow(pivot, cmap='viridis', aspect='auto')
    plt.xlabel('Action')
    plt.ylabel('State')
    plt.colorbar(label='Q-value')
    plt.xticks(ticks=np.arange(len(pivot.columns)), labels=pivot.columns)
    plt.yticks(ticks=np.arange(len(pivot.index)), labels=pivot.index)
    buf = BytesIO()
    plt.tight_layout()
    plt.savefig(buf, format='png')
    plt.close()
    buf.seek(0)
    heatmap_png_base64 = base64.b64encode(buf.read()).decode('utf-8')
    
    # æª¢æŸ¥ç”Ÿæˆçš„åœ–åƒæ˜¯å¦å¤ªå°ï¼ˆå¯èƒ½æœ‰å•é¡Œï¼‰
    if len(heatmap_png_base64) < 100:
        raise HTTPException(status_code=500, detail='Generated heatmap image is too small, possibly corrupted')
    
    return {"heatmap_png_base64": heatmap_png_base64}

@app.get('/{job_id}/optimal-path')
def get_optimal_path(job_id: str):
    qtable_path = os.path.join(JOBS_DIR, job_id, 'q_table.csv')
    map_path = os.path.join(JOBS_DIR, job_id, 'map.json')
    if not os.path.exists(qtable_path) or not os.path.exists(map_path):
        raise HTTPException(status_code=404, detail='Q-Table or map not found')
    df = pd.read_csv(qtable_path)
    with open(map_path, 'r', encoding='utf-8') as f:
        map_data = json.load(f)
    grid = map_data['map'] if 'map' in map_data else None
    if grid is None:
        raise HTTPException(status_code=400, detail='Map format error')
    # æ‰¾èµ·é»
    start = None
    for i, row in enumerate(grid):
        for j, cell in enumerate(row):
            if cell == 'S':
                start = (i, j)
    if start is None:
        raise HTTPException(status_code=400, detail='No start point')
    # ä¾ Q-Table æ‰¾æœ€å„ªè·¯å¾‘
    state = start
    path = [state]
    visited = set()
    actions = ['up', 'down', 'left', 'right']
    for _ in range(100):
        visited.add(state)
        q_vals = df[df['state'] == f"{state[0]},{state[1]}"]
        if q_vals.empty:
            break
        best = q_vals.loc[q_vals['value'].idxmax()]
        action = best['action']
        # ç§»å‹•
        i, j = state
        if action == 'up':
            ni, nj = i-1, j
        elif action == 'down':
            ni, nj = i+1, j
        elif action == 'left':
            ni, nj = i, j-1
        elif action == 'right':
            ni, nj = i, j+1
        else:
            break
        if not (0 <= ni < len(grid) and 0 <= nj < len(grid[0])):
            break
        if grid[ni][nj] == '1':
            break
        state = (ni, nj)
        path.append(state)
        if grid[ni][nj] == 'G':
            break
        if state in visited:
            break
    
    # ç”Ÿæˆæœ€å„ªè·¯å¾‘åœ–ç‰‡
    plt.figure(figsize=(8, 6))
    plt.title('Optimal Path')
    # ç¹ªè£½åœ°åœ–ç¶²æ ¼
    for i, row in enumerate(grid):
        for j, cell in enumerate(row):
            if cell == 'S':
                plt.text(j, i, 'ğŸ§‘â€ğŸŒ¾', ha='center', va='center', fontsize=20)
            elif cell == 'G':
                plt.text(j, i, 'ğŸ', ha='center', va='center', fontsize=20)
            elif cell == 'R':
                plt.text(j, i, 'ğŸª™', ha='center', va='center', fontsize=15)
            elif cell == 'T':
                plt.text(j, i, 'ğŸ•³ï¸', ha='center', va='center', fontsize=15)
            elif cell == '1':
                plt.text(j, i, 'ğŸª¨', ha='center', 
                va='center', fontsize=15)
            else:
                plt.text(j, i, 'Â·', ha='center', va='center', fontsize=10, color='lightgray')
    
    # ç¹ªè£½è·¯å¾‘
    if len(path) > 1:
        path_x = [p[1] for p in path]
        path_y = [p[0] for p in path]
        plt.plot(path_x, path_y, 'r-', linewidth=3, alpha=0.7, label='Optimal Path')
        plt.scatter(path_x, path_y, color='red', s=50, alpha=0.7, marker='o')  # é¿å… colorbar/sci å•é¡Œ
    
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.xlim(-0.5, len(grid[0])-0.5)
    plt.ylim(len(grid)-0.5, -0.5)
    
    buf = BytesIO()
    plt.tight_layout()
    plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')
    plt.close()
    buf.seek(0)
    path_png_base64 = base64.b64encode(buf.read()).decode('utf-8')
    
    return {"optimal_path": path, "path_png_base64": path_png_base64}

def build_analysis_prompt(job_id, user_prompt):
    log_path = os.path.join(JOBS_DIR, job_id, 'log.csv')
    qtable_path = os.path.join(JOBS_DIR, job_id, 'q_table.csv')
    map_path = os.path.join(JOBS_DIR, job_id, 'map.json')
    # å­¸ç¿’æ›²ç·šæ‘˜è¦
    rewards, steps = [], []
    training_summary = {}
    if os.path.exists(log_path):
        df_log = pd.read_csv(log_path)
        rewards = df_log.groupby('episode')['reward'].sum().tolist()[:20]
        steps = df_log.groupby('episode')['step'].max().tolist()[:20]
        
        # è¨ˆç®—è¨“ç·´çµ±è¨ˆ
        total_episodes = df_log['episode'].max()
        avg_reward = df_log.groupby('episode')['reward'].sum().mean()
        avg_steps = df_log.groupby('episode')['step'].max().mean()
        final_reward = df_log.groupby('episode')['reward'].sum().iloc[-1] if len(rewards) > 0 else 0
        final_steps = df_log.groupby('episode')['step'].max().iloc[-1] if len(steps) > 0 else 0
        
        training_summary = {
            'total_episodes': total_episodes,
            'avg_reward': round(avg_reward, 2),
            'avg_steps': round(avg_steps, 2),
            'final_reward': final_reward,
            'final_steps': final_steps,
            'reward_trend': 'ä¸Šå‡' if len(rewards) > 1 and rewards[-1] > rewards[0] else 'ä¸‹é™' if len(rewards) > 1 and rewards[-1] < rewards[0] else 'ç©©å®š',
            'steps_trend': 'ä¸‹é™' if len(steps) > 1 and steps[-1] < steps[0] else 'ä¸Šå‡' if len(steps) > 1 and steps[-1] > steps[0] else 'ç©©å®š'
        }
    # Q-Table ç†±é–€ç‹€æ…‹æ‘˜è¦
    qtable_str = ''
    if os.path.exists(qtable_path):
        df_q = pd.read_csv(qtable_path)
        qtable_top = df_q.sort_values('value', ascending=False).head(10)
        qtable_str = '\n'.join([f"{row['state']}, {row['action']}, {row['value']}" for _, row in qtable_top.iterrows()])
    # æœ€å„ªè·¯å¾‘
    optimal_path = []
    if os.path.exists(qtable_path) and os.path.exists(map_path):
        # ç›´æ¥è¤‡ç”¨ get_optimal_path çš„é‚è¼¯
        with open(map_path, 'r', encoding='utf-8') as f:
            map_data = json.load(f)
        grid = map_data['map'] if 'map' in map_data else None
        if grid:
            df = pd.read_csv(qtable_path)
            start = None
            for i, row in enumerate(grid):
                for j, cell in enumerate(row):
                    if cell == 'S':
                        start = (i, j)
            if start:
                state = start
                path = [state]
                visited = set()
                for _ in range(100):
                    visited.add(state)
                    q_vals = df[df['state'] == f"{state[0]},{state[1]}"]
                    if q_vals.empty:
                        break
                    best = q_vals.loc[q_vals['value'].idxmax()]
                    action = best['action']
                    i, j = state
                    if action == 'up':
                        ni, nj = i-1, j
                    elif action == 'down':
                        ni, nj = i+1, j
                    elif action == 'left':
                        ni, nj = i, j-1
                    elif action == 'right':
                        ni, nj = i, j+1
                    else:
                        break
                    if not (0 <= ni < len(grid) and 0 <= nj < len(grid[0])):
                        break
                    if grid[ni][nj] == '1':
                        break
                    state = (ni, nj)
                    path.append(state)
                    if grid[ni][nj] == 'G':
                        break
                    if state in visited:
                        break
                optimal_path = path
    # åˆä½µ prompt
    prompt = f"""{user_prompt}

## è¨“ç·´æ•¸æ“šåˆ†æ

### è¨“ç·´çµ±è¨ˆæ‘˜è¦
- **ç¸½å›åˆæ•¸**: {training_summary.get('total_episodes', 0)}
- **å¹³å‡çå‹µ**: {training_summary.get('avg_reward', 0)}
- **å¹³å‡æ­¥æ•¸**: {training_summary.get('avg_steps', 0)}
- **æœ€çµ‚çå‹µ**: {training_summary.get('final_reward', 0)}
- **æœ€çµ‚æ­¥æ•¸**: {training_summary.get('final_steps', 0)}
- **çå‹µè¶¨å‹¢**: {training_summary.get('reward_trend', 'æœªçŸ¥')}
- **æ­¥æ•¸è¶¨å‹¢**: {training_summary.get('steps_trend', 'æœªçŸ¥')}

### å­¸ç¿’æ›²ç·šæ•¸æ“šï¼ˆå‰20å›åˆï¼‰
- **çå‹µåºåˆ—**: {rewards}
- **æ­¥æ•¸åºåˆ—**: {steps}

### Q-Table åˆ†æ
**æœ€é«˜åƒ¹å€¼ç‹€æ…‹-å‹•ä½œå°ï¼ˆå‰10ç­†ï¼‰:**
{qtable_str}

### æœ€å„ªè·¯å¾‘åˆ†æ
**AIé¸æ“‡çš„æœ€å„ªè·¯å¾‘**: {optimal_path}

## åˆ†æè¦æ±‚

è«‹æ ¹æ“šä»¥ä¸Šæ•¸æ“šé€²è¡Œè©³ç´°åˆ†æï¼Œä¸¦æä¾›ä»¥ä¸‹å…§å®¹ï¼š

### 1. å­¸ç¿’æ•ˆæœè©•ä¼°
- åˆ†æå­¸ç¿’æ›²ç·šçš„è¶¨å‹¢ï¼ˆçå‹µå’Œæ­¥æ•¸è®ŠåŒ–ï¼‰
- è©•ä¼°AIæ˜¯å¦æˆåŠŸå­¸ç¿’åˆ°æœ‰æ•ˆç­–ç•¥
- åˆ¤æ–·è¨“ç·´æ˜¯å¦æ”¶æ–‚
- è©•ä¼°æœ€çµ‚æ€§èƒ½è¡¨ç¾

### 2. å•é¡Œè¨ºæ–·
- è­˜åˆ¥è¨“ç·´éç¨‹ä¸­çš„å•é¡Œï¼ˆå¦‚å¾ªç’°ã€æ”¶æ–‚å¤±æ•—ã€æ¢ç´¢ä¸è¶³ç­‰ï¼‰
- åˆ†æQ-Tableçš„å­¸ç¿’è³ªé‡ï¼ˆæ˜¯å¦æœ‰æ˜é¡¯çš„åƒ¹å€¼åˆ†å¸ƒï¼‰
- è©•ä¼°æœ€å„ªè·¯å¾‘çš„åˆç†æ€§ï¼ˆæ˜¯å¦èƒ½åˆ°é”ç›®æ¨™ï¼‰
- æª¢æŸ¥æ˜¯å¦å­˜åœ¨éæ“¬åˆæˆ–æ¬ æ“¬åˆ

### 3. æ”¹é€²å»ºè­°
- é‡å°ç™¼ç¾çš„å•é¡Œæä¾›å…·é«”æ”¹é€²æ–¹æ¡ˆ
- å»ºè­°åƒæ•¸èª¿æ•´æ–¹å‘ï¼ˆå­¸ç¿’ç‡ã€æŠ˜æ‰£å› å­ã€æ¢ç´¢ç‡ç­‰ï¼‰
- æä¾›è¨“ç·´ç­–ç•¥å„ªåŒ–å»ºè­°
- å»ºè­°åˆé©çš„è¨“ç·´å›åˆæ•¸

### 4. ç®—æ³•ç‰¹æ€§åˆ†æ
- åˆ†æç•¶å‰ç®—æ³•çš„å„ªç¼ºé»
- èˆ‡å…¶ä»–å¼·åŒ–å­¸ç¿’ç®—æ³•çš„æ¯”è¼ƒ
- é©ç”¨å ´æ™¯è©•ä¼°
- ç®—æ³•é¸æ“‡å»ºè­°

### 5. ç¸½çµèˆ‡è©•åˆ†
- æ•´é«”è¨“ç·´æ•ˆæœè©•åˆ†ï¼ˆ1-10åˆ†ï¼‰
- ä¸»è¦æˆå°±å’Œå•é¡Œç¸½çµ
- å¯¦ç”¨æ€§è©•ä¼°

è«‹ä»¥çµæ§‹åŒ–çš„æ–¹å¼å‘ˆç¾åˆ†æçµæœï¼Œä½¿ç”¨æ¸…æ™°çš„æ¨™é¡Œå’Œè¦é»ï¼Œä¸¦åŒæ™‚è¼¸å‡º markdown èˆ‡ html ç‰ˆæœ¬ã€‚"""
    return prompt

@app.post('/{job_id}/analyze-and-save')
def analyze_and_save(job_id: str, user_prompt: str = Body(..., embed=True)):
    job_dir = os.path.join(JOBS_DIR, job_id)
    if not os.path.exists(job_dir):
        raise HTTPException(status_code=404, detail='Job not found')
    
    # å‰µå»ºåˆ†æè¨˜éŒ„ç›®éŒ„
    analysis_log_dir = os.path.join(job_dir, 'analysis_logs')
    os.makedirs(analysis_log_dir, exist_ok=True)
    
    # ç”Ÿæˆæ™‚é–“æˆ³
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # è®€å– AI è¨­å®š
    with open('settings.json', 'r', encoding='utf-8') as f:
        settings = json.load(f)
    system_prompt = settings.get('system_prompt', '')
    api_key = settings.get('api_key', '')
    model_name = settings.get('model_name', '')
    
    # è‡ªå‹•åˆä½µ prompt
    prompt = build_analysis_prompt(job_id, user_prompt)
    
    # è¨˜éŒ„å®Œæ•´çš„è«‹æ±‚ä¿¡æ¯
    request_log = {
        'timestamp': timestamp,
        'job_id': job_id,
        'user_prompt': user_prompt,
        'system_prompt': system_prompt,
        'model_name': model_name,
        'full_prompt': system_prompt + "\n" + prompt,
        'request_data': {
            "contents": [
                {"role": "user", "parts": [{"text": system_prompt + "\n" + prompt}]}
            ]
        }
    }
    
    # å„²å­˜è«‹æ±‚è¨˜éŒ„
    request_log_path = os.path.join(analysis_log_dir, f'request_{timestamp}.json')
    with open(request_log_path, 'w', encoding='utf-8') as f:
        json.dump(request_log, f, ensure_ascii=False, indent=2)
    
    # å‘¼å« Gemini API
    url = f'https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={api_key}'
    headers = {'Content-Type': 'application/json'}
    data = {
        "contents": [
            {"role": "user", "parts": [{"text": system_prompt + "\n" + prompt}]}
        ]
    }
    
    resp = requests.post(url, headers=headers, json=data)
    
    # è¨˜éŒ„APIéŸ¿æ‡‰
    response_log = {
        'timestamp': timestamp,
        'job_id': job_id,
        'status_code': resp.status_code,
        'response_headers': dict(resp.headers),
        'response_text': resp.text,
        'success': resp.status_code == 200
    }
    
    # å„²å­˜éŸ¿æ‡‰è¨˜éŒ„
    response_log_path = os.path.join(analysis_log_dir, f'response_{timestamp}.json')
    with open(response_log_path, 'w', encoding='utf-8') as f:
        json.dump(response_log, f, ensure_ascii=False, indent=2)
    
    if resp.status_code != 200:
        raise HTTPException(status_code=500, detail=f'Gemini API error: {resp.text}')
    
    gemini_content = resp.json()['candidates'][0]['content']['parts'][0]['text']
    
    # è¨˜éŒ„åŸå§‹AIå›è¦†
    raw_response_log = {
        'timestamp': timestamp,
        'job_id': job_id,
        'raw_ai_response': gemini_content,
        'response_length': len(gemini_content)
    }
    
    # å„²å­˜åŸå§‹å›è¦†è¨˜éŒ„
    raw_response_log_path = os.path.join(analysis_log_dir, f'raw_response_{timestamp}.json')
    with open(raw_response_log_path, 'w', encoding='utf-8') as f:
        json.dump(raw_response_log, f, ensure_ascii=False, indent=2)
    
    # å„²å­˜åŸå§‹AIå›è¦†ç‚ºæ–‡æœ¬æ–‡ä»¶
    raw_response_text_path = os.path.join(analysis_log_dir, f'raw_response_{timestamp}.txt')
    with open(raw_response_text_path, 'w', encoding='utf-8') as f:
        f.write(gemini_content)
    
    # è™•ç† markdown å…§å®¹
    md_match = re.search(r"```markdown\s*([\s\S]+?)```", gemini_content)
    md_content = md_match.group(1).strip() if md_match else gemini_content
    
    # è¨˜éŒ„ markdown è™•ç†éç¨‹
    md_processing_log = {
        'timestamp': timestamp,
        'job_id': job_id,
        'has_markdown_block': md_match is not None,
        'markdown_content_length': len(md_content),
        'markdown_content_preview': md_content[:500] + '...' if len(md_content) > 500 else md_content
    }
    
    # å„²å­˜ markdown è™•ç†è¨˜éŒ„
    md_processing_log_path = os.path.join(analysis_log_dir, f'md_processing_{timestamp}.json')
    with open(md_processing_log_path, 'w', encoding='utf-8') as f:
        json.dump(md_processing_log, f, ensure_ascii=False, indent=2)
    
    # å„²å­˜ .md
    md_path = os.path.join(job_dir, 'analysis.md')
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    # è™•ç† HTML å…§å®¹
    html_match = re.search(r"```html\s*([\s\S]+?)```", gemini_content)
    if html_match:
        html_content = html_match.group(1).strip()
        html_source = 'ai_generated'
    else:
        html_content = markdown2.markdown(md_content)
        html_source = 'markdown_converted'
    
    # è¨˜éŒ„ HTML è™•ç†éç¨‹
    html_processing_log = {
        'timestamp': timestamp,
        'job_id': job_id,
        'html_source': html_source,
        'has_html_block': html_match is not None,
        'html_content_length': len(html_content),
        'html_content_preview': html_content[:500] + '...' if len(html_content) > 500 else html_content
    }
    
    # å„²å­˜ HTML è™•ç†è¨˜éŒ„
    html_processing_log_path = os.path.join(analysis_log_dir, f'html_processing_{timestamp}.json')
    with open(html_processing_log_path, 'w', encoding='utf-8') as f:
        json.dump(html_processing_log, f, ensure_ascii=False, indent=2)
    
    # å„²å­˜ .html
    html_path = os.path.join(job_dir, 'analysis.html')
    with open(html_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    # å‰µå»ºåˆ†ææ‘˜è¦è¨˜éŒ„
    analysis_summary = {
        'timestamp': timestamp,
        'job_id': job_id,
        'user_prompt': user_prompt,
        'analysis_files_created': {
            'analysis.md': os.path.exists(md_path),
            'analysis.html': os.path.exists(html_path),
            'request_log': os.path.exists(request_log_path),
            'response_log': os.path.exists(response_log_path),
            'raw_response_log': os.path.exists(raw_response_log_path),
            'raw_response_text': os.path.exists(raw_response_text_path),
            'md_processing_log': os.path.exists(md_processing_log_path),
            'html_processing_log': os.path.exists(html_processing_log_path)
        },
        'content_stats': {
            'raw_response_length': len(gemini_content),
            'markdown_length': len(md_content),
            'html_length': len(html_content),
            'html_source': html_source
        }
    }
    
    # å„²å­˜åˆ†ææ‘˜è¦
    summary_log_path = os.path.join(analysis_log_dir, f'analysis_summary_{timestamp}.json')
    with open(summary_log_path, 'w', encoding='utf-8') as f:
        json.dump(analysis_summary, f, ensure_ascii=False, indent=2)
    
    return {"message": "Analysis saved.", "md": md_content, "html": html_content}

@app.get('/{job_id}/report')
def get_analysis_report(job_id: str):
    report_path = os.path.join(JOBS_DIR, job_id, 'analysis.md')
    if not os.path.exists(report_path):
        raise HTTPException(status_code=404, detail='Analysis report not found')
    with open(report_path, 'r', encoding='utf-8') as f:
        content = f.read()
    return {"content": content}

@app.get('/{job_id}/analysis.html')
def get_analysis_html(job_id: str):
    """ç›´æ¥è¿”å› analysis.html çš„å…§å®¹ä¾›å‰ç«¯åµŒå…¥"""
    html_path = os.path.join(JOBS_DIR, job_id, 'analysis.html')
    if not os.path.exists(html_path):
        raise HTTPException(status_code=404, detail='Analysis HTML not found')
    with open(html_path, 'r', encoding='utf-8') as f:
        content = f.read()
    return {"html_content": content}

@app.get('/{job_id}/config.json')
def get_job_config(job_id: str):
    """è¿”å› job çš„é…ç½®ä¿¡æ¯"""
    config_path = os.path.join(JOBS_DIR, job_id, 'config.json')
    if not os.path.exists(config_path):
        raise HTTPException(status_code=404, detail='Job config not found')
    with open(config_path, 'r', encoding='utf-8') as f:
        config_data = json.load(f)
    return config_data

@app.get('/{job_id}/rule.json')
def get_job_rule(job_id: str):
    """è¿”å› job å°ˆç”¨çš„ rule.json"""
    rule_path = os.path.join(JOBS_DIR, job_id, 'rule.json')
    if not os.path.exists(rule_path):
        raise HTTPException(status_code=404, detail='Job rule not found')
    with open(rule_path, 'r', encoding='utf-8') as f:
        rule_data = json.load(f)
    return rule_data

@app.get('/{job_id}/map.json')
def get_job_map(job_id: str):
    """è¿”å› job å°ˆç”¨çš„ map.json"""
    map_path = os.path.join(JOBS_DIR, job_id, 'map.json')
    if not os.path.exists(map_path):
        raise HTTPException(status_code=404, detail='Job map not found')
    with open(map_path, 'r', encoding='utf-8') as f:
        map_data = json.load(f)
    return map_data

@app.get('/{job_id}/verify')
def verify_training_api(job_id: str):
    log_path = os.path.join(JOBS_DIR, job_id, 'log.csv')
    if not os.path.exists(log_path):
        raise HTTPException(status_code=404, detail='Log not found')
    try:
        result = subprocess.run(['python', 'verify_training.py', log_path], capture_output=True, text=True, timeout=10)
        verify_ok = '[OK]' in result.stdout
        return {
            'verify_ok': verify_ok,
            'verify_output': result.stdout,
            'returncode': result.returncode
        }
    except Exception as e:
        return {
            'verify_ok': False,
            'verify_output': f'é©—è­‰éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}',
            'returncode': -1
        }

 